COMPLETE TRAINING & RL MODEL EVALUATION RESULTS
===============================================
Job ID: 2670233
Control System: double_integrator
Started: Fri Aug 15 16:34:11 EDT 2025
Node: node2906

TRAINING PIPELINE PHASES:
✅ SFT Training: COMPLETED
✅ GRPO Training: COMPLETED (or SFT-only if GRPO failed)
✅ RL Model Evaluation: COMPLETED
✅ Additional Visualizations: COMPLETED
✅ Summary Report: COMPLETED

OUTPUT LOCATIONS:
================
- Trained Models: models/single_system/double_integrator/[sft|grpo]/latest/
- RL Model Evaluation: figures/job_2670233/model_comparison_results/
- LQR Verification: figures/job_2670233/lqr_verification/
- Job Logs: logs/train_eval_complete_2670233.out/err

RL MODEL EVALUATION OUTPUTS:
============================
- Success Rate Comparison: */model_comparison_results/success_rate_comparison.png
- Trajectory Comparison: */model_comparison_results/trajectory_comparison.png
- Performance Analysis: */model_comparison_results/performance_analysis.png
- Numerical Results: */model_comparison_results/evaluation_results.json

COMPARISON ANALYSIS:
===================
The evaluation compares:
1. SFT Model (Supervised Fine-Tuning)
2. GRPO Model (Group Relative Policy Optimization) 
3. LQR Optimal Control (Baseline)

Each model is tested on diverse initial conditions to assess:
- Success rate (reaching target region ||state|| < 0.1)
- Trajectory quality compared to optimal control
- Prediction reliability (valid format rate)
- Control effort and smoothness

TRAINING CONFIGURATION:
======================
- System: double_integrator
- GPU: H100 (80GB HBM3)
- Memory: 100GB allocated
- Base Model: Qwen3-4B with LoRA adapters
- Time Limit: 4 hours

The complete training and RL model evaluation pipeline finished successfully!
This provides comprehensive analysis of learned control policies vs optimal control.
