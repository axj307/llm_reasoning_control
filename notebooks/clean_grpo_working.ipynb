{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean GRPO Training - Working Implementation\n",
    "\n",
    "This notebook combines your proven working GRPO approach with your current dataset.\n",
    "It bypasses all modular complexity and focuses on getting GRPO training to work immediately.\n",
    "\n",
    "**Based on**: `archive/Qwen3_(4B)-GRPO_control.ipynb` (your working notebook)\n",
    "**Dataset**: Uses your existing `datasets/di_train.pkl` and `datasets/di_eval.pkl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Edit these parameters as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Configuration:\n",
      "   Mode: Small dataset\n",
      "   Training samples: 50\n",
      "   SFT max steps: 10\n",
      "   GRPO max steps: 10\n",
      "   Model: Qwen3-4B-Base, LoRA rank 16\n",
      "   Max sequence length: 1024\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - EDIT THESE PARAMETERS\n",
    "# =============================================================================\n",
    "\n",
    "# Training mode\n",
    "USE_SMALL_DATASET = True  # Set to False for full dataset\n",
    "\n",
    "# Dataset sizes (when USE_SMALL_DATASET=True)\n",
    "TRAIN_SAMPLES = 50   # Number of training samples\n",
    "EVAL_SAMPLES = 10    # Number of evaluation samples\n",
    "\n",
    "# Model configuration\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "LORA_RANK = 16\n",
    "GPU_MEMORY_UTIL = 0.4\n",
    "\n",
    "# SFT Training (pre-training phase)\n",
    "SFT_EPOCHS = 1\n",
    "SFT_MAX_STEPS = 10  # Set to None to use epochs\n",
    "SFT_BATCH_SIZE = 2\n",
    "\n",
    "# GRPO Training (main training phase)\n",
    "GRPO_MAX_STEPS = 10\n",
    "GRPO_BATCH_SIZE = 1      # Conservative to avoid tensor issues\n",
    "GRPO_NUM_GENERATIONS = 1  # Conservative to avoid tensor issues\n",
    "GRPO_MAX_COMPLETION = 512 # Conservative completion length\n",
    "GRPO_TEMPERATURE = 0.8    # Stable temperature\n",
    "\n",
    "# Control system parameters\n",
    "REASONING_START = \"<REASONING>\"\n",
    "REASONING_END = \"</REASONING>\"\n",
    "SOLUTION_START = \"<CONTROLS>\"\n",
    "SOLUTION_END = \"</CONTROLS>\"\n",
    "DT = 0.1  # Time step\n",
    "STEPS = 50  # Number of control steps\n",
    "\n",
    "print(f\"üéØ Configuration:\")\n",
    "print(f\"   Mode: {'Small dataset' if USE_SMALL_DATASET else 'Full dataset'}\")\n",
    "if USE_SMALL_DATASET:\n",
    "    print(f\"   Training samples: {TRAIN_SAMPLES}\")\n",
    "    print(f\"   SFT max steps: {SFT_MAX_STEPS}\")\n",
    "    print(f\"   GRPO max steps: {GRPO_MAX_STEPS}\")\n",
    "print(f\"   Model: Qwen3-4B-Base, LoRA rank {LORA_RANK}\")\n",
    "print(f\"   Max sequence length: {MAX_SEQ_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Selected GPU: 0\n",
      "‚úÖ Basic setup complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(3407)\n",
    "np.random.seed(3407)\n",
    "random.seed(3407)\n",
    "\n",
    "# GPU selection\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus > 0:\n",
    "    chosen_gpu = random.randint(0, num_gpus - 1)\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(chosen_gpu)\n",
    "    print(f\"üñ•Ô∏è  Selected GPU: {chosen_gpu}\")\n",
    "else:\n",
    "    print(\"‚ùå No GPUs available.\")\n",
    "\n",
    "print(\"‚úÖ Basic setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model (Working Notebook Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-31 17:47:49 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-31 17:47:49 [__init__.py:239] Automatically detected platform cuda.\n",
      "üöÄ Loading model (non-vLLM approach to avoid conflicts)...\n",
      "==((====))==  Unsloth 2025.6.1: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully (standard mode)\n",
      "   Model type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "   Has chat template: True\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "print(\"üöÄ Loading model (non-vLLM approach to avoid conflicts)...\")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Qwen3-4B-Base\",\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference=False,  # Disable vLLM to avoid conflicts\n",
    "    max_lora_rank=LORA_RANK,\n",
    "    # gpu_memory_utilization removed as it's vLLM specific\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=LORA_RANK,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=LORA_RANK * 2,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully (standard mode)\")\n",
    "print(f\"   Model type: {type(model)}\")\n",
    "print(f\"   Has chat template: {hasattr(tokenizer, 'chat_template')}\")\n",
    "\n",
    "# Enable training mode\n",
    "model = FastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Chat Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Setting up chat template...\n",
      "‚úÖ Chat template configured\n",
      "   System prompt length: 454 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Setting up chat template...\")\n",
    "\n",
    "# System prompt for double integrator control\n",
    "total_time = DT * STEPS\n",
    "system_prompt = f\"\"\"You are a control systems expert.\n",
    "Given a double integrator system (·∫ç = u) with initial position and velocity,\n",
    "generate a sequence of {STEPS} control inputs to reach the origin (0,0) in exactly {total_time:.2f} seconds.\n",
    "Position and velocity must stay within [-1, 1], and control inputs must be within [-3, 3].\n",
    "Explain your approach between {REASONING_START} and {REASONING_END}.\n",
    "Then provide exactly {STEPS} control values as a comma-separated list between {SOLUTION_START} and {SOLUTION_END}.\"\"\"\n",
    "\n",
    "# Chat template (exact from working notebook)\n",
    "chat_template = \\\n",
    "    \"{% if messages[0]['role'] == 'system' %}\"\\\n",
    "        \"{{ messages[0]['content'] + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages[1:] %}\"\\\n",
    "    \"{% else %}\"\\\n",
    "        \"{{ '{system_prompt}' + eos_token }}\"\\\n",
    "        \"{% set loop_messages = messages %}\"\\\n",
    "    \"{% endif %}\"\\\n",
    "    \"{% for message in loop_messages %}\"\\\n",
    "        \"{% if message['role'] == 'user' %}\"\\\n",
    "            \"{{ message['content'] }}\"\\\n",
    "        \"{% elif message['role'] == 'assistant' %}\"\\\n",
    "            \"{{ message['content'] + eos_token }}\"\\\n",
    "        \"{% endif %}\"\\\n",
    "    \"{% endfor %}\"\\\n",
    "    \"{% if add_generation_prompt %}{{ '{reasoning_start}' }}\"\\\n",
    "    \"{% endif %}\"\n",
    "\n",
    "# Replace placeholders\n",
    "chat_template = chat_template\\\n",
    "    .replace(\"'{system_prompt}'\", f\"'{system_prompt}'\")\\\n",
    "    .replace(\"'{reasoning_start}'\", f\"'{REASONING_START}'\")\n",
    "\n",
    "tokenizer.chat_template = chat_template\n",
    "\n",
    "print(\"‚úÖ Chat template configured\")\n",
    "print(f\"   System prompt length: {len(system_prompt)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset...\n",
      "‚úÖ Loaded 1800 train and 200 eval samples\n",
      "   Filtered: 1800 train, 200 eval for double_integrator\n",
      "   Using subset: 50 train, 10 eval\n",
      "   Sample keys: ['system_type', 'initial_state', 'controls', 'system_prompt', 'problem', 'reasoning', 'complete_output', 'messages']\n",
      "   Messages structure: 3 messages\n",
      "     0: system - 454 chars\n",
      "     1: user - 209 chars\n",
      "     2: assistant - 1298 chars\n"
     ]
    }
   ],
   "source": [
    "print(\"üìÇ Loading dataset...\")\n",
    "\n",
    "# Load your existing dataset (correct path from notebooks directory)\n",
    "try:\n",
    "    with open(\"../datasets/di_train.pkl\", \"rb\") as f:\n",
    "        train_data = pickle.load(f)\n",
    "    \n",
    "    with open(\"../datasets/di_eval.pkl\", \"rb\") as f:\n",
    "        eval_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(train_data)} train and {len(eval_data)} eval samples\")\n",
    "    \n",
    "    # Filter to double integrator if needed\n",
    "    train_data = [x for x in train_data if x.get(\"system_type\") == \"double_integrator\"]\n",
    "    eval_data = [x for x in eval_data if x.get(\"system_type\") == \"double_integrator\"]\n",
    "    \n",
    "    print(f\"   Filtered: {len(train_data)} train, {len(eval_data)} eval for double_integrator\")\n",
    "    \n",
    "    # Use subset if requested\n",
    "    if USE_SMALL_DATASET:\n",
    "        train_data = train_data[:TRAIN_SAMPLES]\n",
    "        eval_data = eval_data[:EVAL_SAMPLES]\n",
    "        print(f\"   Using subset: {len(train_data)} train, {len(eval_data)} eval\")\n",
    "    \n",
    "    # Check data format\n",
    "    sample = train_data[0]\n",
    "    print(f\"   Sample keys: {list(sample.keys())}\")\n",
    "    if \"messages\" in sample:\n",
    "        print(f\"   Messages structure: {len(sample['messages'])} messages\")\n",
    "        for i, msg in enumerate(sample[\"messages\"]):\n",
    "            print(f\"     {i}: {msg['role']} - {len(msg['content'])} chars\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load dataset: {e}\")\n",
    "    print(\"üí° Make sure ../datasets/di_train.pkl and ../datasets/di_eval.pkl exist\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training (SFT Phase)\n",
    "Brief SFT training to prepare the model for GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Starting SFT pre-training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c42ea4d186241e2b984193239d6edf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950572cc78874e66b215f05753b80cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SFT datasets: 50 train, 10 eval\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 50. Reducing num_proc to 50 for dataset of size 50.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b6f3041d7c4eeb9536f3fe3eb1497f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=50):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 10. Reducing num_proc to 10 for dataset of size 10.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7dd877da164067820f672e51953206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=10):   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     26\u001b[39m sft_config = SFTConfig(\n\u001b[32m     27\u001b[39m     dataset_text_field=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m     per_device_train_batch_size=SFT_BATCH_SIZE,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     save_steps=\u001b[32m1000\u001b[39m,  \u001b[38;5;66;03m# Don't save during short training\u001b[39;00m\n\u001b[32m     42\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Create SFT trainer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m sft_trainer = \u001b[43mSFTTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_train_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_eval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43msft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   Running SFT training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     54\u001b[39m sft_result = sft_trainer.train()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/trainer.py:210\u001b[39m, in \u001b[36m_backwards_compatible_trainer.<locals>.new_init\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m] = config\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/amitjain/project/Unsloth/llm_reasoning_control/notebooks/unsloth_compiled_cache/UnslothSFTTrainer.py:1026\u001b[39m, in \u001b[36mUnslothSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func, **kwargs)\u001b[39m\n\u001b[32m   1023\u001b[39m fix_untrained_tokens(model, tokenizer, train_dataset, IGNORED_TOKENIZER_NAMES, eps = \u001b[32m1e-16\u001b[39m)\n\u001b[32m   1024\u001b[39m fix_zero_training_loss(model, tokenizer, train_dataset)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_cls_and_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m    \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatting_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mneftune_hook_handle\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28mself\u001b[39m.neftune_hook_handle.remove()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/amitjain/project/Unsloth/llm_reasoning_control/notebooks/unsloth_compiled_cache/UnslothSFTTrainer.py:492\u001b[39m, in \u001b[36m_UnslothSFTTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics, peft_config, formatting_func)\u001b[39m\n\u001b[32m    486\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m optimizer_cls_and_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    487\u001b[39m         warnings.warn(\n\u001b[32m    488\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe `optimizer_cls_and_kwargs` argument is only available for `transformers>=4.47.0`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    489\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe default optimizer will be used. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    490\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mRemove the `optimizer_cls_and_kwargs` or upgrade to `transformers>=4.47.0`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_loss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreprocess_logits_for_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msuper_init_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, \u001b[33m\"\u001b[39m\u001b[33madd_model_tags\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/transformers/trainer.py:697\u001b[39m, in \u001b[36mTrainer.__init__\u001b[39m\u001b[34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, optimizer_cls_and_kwargs, preprocess_logits_for_metrics)\u001b[39m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m.data_collator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.data_collator, \u001b[33m\"\u001b[39m\u001b[33mcollate_batch\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[32m    695\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThe `data_collator` should be a simple callable (function, class with `__call__`).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m697\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m args.max_steps > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m:\n\u001b[32m    698\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mmax_steps is given, it will override any value given in num_train_epochs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m train_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_length(train_dataset) \u001b[38;5;129;01mand\u001b[39;00m args.max_steps <= \u001b[32m0\u001b[39m:\n",
      "\u001b[31mTypeError\u001b[39m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"üìö Starting SFT pre-training...\")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "# Format data for SFT\n",
    "def format_for_sft(example):\n",
    "    messages = example[\"messages\"]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Create datasets\n",
    "sft_train_dataset = Dataset.from_list(train_data)\n",
    "sft_train_dataset = sft_train_dataset.map(format_for_sft)\n",
    "\n",
    "sft_eval_dataset = Dataset.from_list(eval_data)\n",
    "sft_eval_dataset = sft_eval_dataset.map(format_for_sft)\n",
    "\n",
    "print(f\"   SFT datasets: {len(sft_train_dataset)} train, {len(sft_eval_dataset)} eval\")\n",
    "\n",
    "# SFT configuration\n",
    "sft_config = SFTConfig(\n",
    "    dataset_text_field=\"text\",\n",
    "    per_device_train_batch_size=SFT_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=5,\n",
    "    num_train_epochs=SFT_EPOCHS if SFT_MAX_STEPS is None else None,\n",
    "    max_steps=SFT_MAX_STEPS,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=max(1, (SFT_MAX_STEPS or 10) // 5),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "    report_to=\"none\",  # Disable wandb for now\n",
    "    output_dir=\"./sft_output\",\n",
    "    save_steps=1000,  # Don't save during short training\n",
    ")\n",
    "\n",
    "# Create SFT trainer\n",
    "sft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=sft_train_dataset,\n",
    "    eval_dataset=sft_eval_dataset,\n",
    "    args=sft_config,\n",
    ")\n",
    "\n",
    "print(\"   Running SFT training...\")\n",
    "sft_result = sft_trainer.train()\n",
    "\n",
    "print(\"‚úÖ SFT pre-training completed!\")\n",
    "print(f\"   Final loss: {sft_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRPO Training (Main Phase)\n",
    "This is the main GRPO training using the exact approach from your working notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéÆ Setting up GRPO training (standard approach)...\")\n",
    "\n",
    "# Clear memory\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n",
    "# Format data for GRPO (exact from working notebook)\n",
    "def format_for_grpo(data):\n",
    "    formatted = []\n",
    "    for example in data:\n",
    "        messages = example[\"messages\"]\n",
    "        prompt_messages = messages[:-1]  # Exclude assistant response\n",
    "        \n",
    "        # Extract control answer\n",
    "        controls = example.get(\"controls\", [])\n",
    "        if isinstance(controls, list):\n",
    "            answer = \", \".join([f\"{u:.3f}\" for u in controls])\n",
    "        else:\n",
    "            answer = str(controls)\n",
    "        \n",
    "        formatted.append({\n",
    "            \"prompt\": prompt_messages,\n",
    "            \"answer\": answer,\n",
    "            \"Messages\": messages\n",
    "        })\n",
    "    return formatted\n",
    "\n",
    "# Format datasets\n",
    "grpo_train_data = format_for_grpo(train_data)\n",
    "grpo_eval_data = format_for_grpo(eval_data)\n",
    "\n",
    "grpo_train_dataset = Dataset.from_list(grpo_train_data)\n",
    "grpo_eval_dataset = Dataset.from_list(grpo_eval_data)\n",
    "\n",
    "print(f\"   GRPO datasets: {len(grpo_train_dataset)} train, {len(grpo_eval_dataset)} eval\")\n",
    "\n",
    "# GRPO configuration (standard approach without vLLM)\n",
    "grpo_config = GRPOConfig(\n",
    "    # No vLLM sampling params - use standard generation\n",
    "    temperature=GRPO_TEMPERATURE,\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    optim=\"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=GRPO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_new_tokens=GRPO_MAX_COMPLETION,  # Use max_new_tokens instead of max_completion_length\n",
    "    max_steps=GRPO_MAX_STEPS,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",  # Disable wandb for now\n",
    "    output_dir=\"./grpo_output\",\n",
    "    dataloader_drop_last=True,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "print(f\"   GRPO config (standard mode):\")\n",
    "print(f\"     Batch size: {GRPO_BATCH_SIZE}\")\n",
    "print(f\"     Max new tokens: {GRPO_MAX_COMPLETION}\")\n",
    "print(f\"     Max steps: {GRPO_MAX_STEPS}\")\n",
    "print(f\"     Temperature: {GRPO_TEMPERATURE}\")\n",
    "print(\"   ‚ö†Ô∏è  Using standard generation (not vLLM) to avoid conflicts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Reward Functions (Exact from Working Notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Setting up reward functions (standard generation)...\")\n",
    "\n",
    "# Define regex pattern\n",
    "solution_end_regex = rf\"{re.escape(SOLUTION_END)}[\\s]{{0,}}\" + \\\n",
    "    f\"(?:{re.escape(tokenizer.eos_token)})?\"\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"{re.escape(REASONING_END)}.*?\"\\\n",
    "    rf\"{re.escape(SOLUTION_START)}(.+?){solution_end_regex}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags=re.MULTILINE | re.DOTALL\n",
    ")\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    \"\"\"Reward function: exact format matching (standard generation).\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        # Standard generation returns text directly\n",
    "        response = completion if isinstance(completion, str) else str(completion)\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    \"\"\"Reward function: approximate format matching (standard generation).\"\"\"\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion if isinstance(completion, str) else str(completion)\n",
    "        score += 0.5 if response.count(REASONING_END) == 1 else -1.0\n",
    "        score += 0.5 if response.count(SOLUTION_START) == 1 else -1.0\n",
    "        score += 0.5 if response.count(SOLUTION_END) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def evaluate_control_sequence(prompts, completions, answer, **kwargs):\n",
    "    \"\"\"Enhanced evaluation of control sequences (standard generation).\"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for completion, true_answer in zip(completions, answer):\n",
    "        score = 0\n",
    "        response = completion if isinstance(completion, str) else str(completion)\n",
    "        \n",
    "        # Extract control sequence\n",
    "        control_match = re.search(rf\"{SOLUTION_START}(.*?){SOLUTION_END}\", response, re.DOTALL)\n",
    "        if control_match is None:\n",
    "            scores.append(-2.0)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Parse control values\n",
    "            control_text = control_match.group(1).strip()\n",
    "            control_values = [float(x.strip()) for x in control_text.split(',')]\n",
    "            \n",
    "            # Check constraints\n",
    "            if len(control_values) == STEPS:\n",
    "                score += 1.0\n",
    "            else:\n",
    "                score -= 1.0\n",
    "                \n",
    "            if all(-3 <= u <= 3 for u in control_values):\n",
    "                score += 1.0\n",
    "            else:\n",
    "                score -= 2.0\n",
    "            \n",
    "            # Check for smoothness (LQR characteristic)\n",
    "            if len(control_values) > 1:\n",
    "                diffs = [abs(control_values[i] - control_values[i-1]) for i in range(1, len(control_values))]\n",
    "                if max(diffs) < 1.5:  # Smooth control changes\n",
    "                    score += 1.5\n",
    "            \n",
    "            # Try to extract initial state from prompt for simulation\n",
    "            try:\n",
    "                # Handle different prompt formats\n",
    "                if isinstance(prompts, list) and len(prompts) > 0:\n",
    "                    if isinstance(prompts[0], list):\n",
    "                        problem_text = prompts[0][-1].get(\"content\", \"\")\n",
    "                    elif isinstance(prompts[0], dict):\n",
    "                        problem_text = prompts[0].get(\"content\", \"\")\n",
    "                    else:\n",
    "                        problem_text = str(prompts[0])\n",
    "                else:\n",
    "                    problem_text = str(prompts)\n",
    "                \n",
    "                initial_match = re.search(r\"position=([-\\d\\.]+), velocity=([-\\d\\.]+)\", problem_text)\n",
    "                if initial_match:\n",
    "                    x0 = float(initial_match.group(1))\n",
    "                    v0 = float(initial_match.group(2))\n",
    "                    \n",
    "                    # Simulate trajectory\n",
    "                    x, v = x0, v0\n",
    "                    valid_trajectory = True\n",
    "                    \n",
    "                    for u in control_values:\n",
    "                        v = v + u * DT\n",
    "                        x = x + v * DT\n",
    "                        \n",
    "                        if not (-1 <= x <= 1 and -1 <= v <= 1):\n",
    "                            valid_trajectory = False\n",
    "                            break\n",
    "                    \n",
    "                    # Reward valid trajectory\n",
    "                    if valid_trajectory:\n",
    "                        score += 1.0\n",
    "                    else:\n",
    "                        score -= 1.0\n",
    "                    \n",
    "                    # Reward based on final error\n",
    "                    final_error = np.sqrt(x**2 + v**2)\n",
    "                    if final_error < 0.1:\n",
    "                        score += 3.0\n",
    "                    elif final_error < 0.2:\n",
    "                        score += 2.0\n",
    "                    elif final_error < 0.5:\n",
    "                        score += 1.0\n",
    "                    else:\n",
    "                        score -= 1.0\n",
    "            except Exception:\n",
    "                # If simulation fails, just use format/constraint scores\n",
    "                pass\n",
    "            \n",
    "            scores.append(score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            scores.append(-2.0)\n",
    "            \n",
    "    return scores\n",
    "\n",
    "# Combine reward functions\n",
    "reward_functions = [\n",
    "    match_format_exactly,\n",
    "    match_format_approximately,\n",
    "    evaluate_control_sequence,\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ {len(reward_functions)} reward functions ready (standard mode)\")\n",
    "print(\"   Adapted for standard generation (non-vLLM)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GRPO Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ Starting GRPO training...\")\n",
    "\n",
    "# Create GRPO trainer\n",
    "grpo_trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer, \n",
    "    reward_funcs=reward_functions,\n",
    "    args=grpo_config,\n",
    "    train_dataset=grpo_train_dataset,\n",
    "    eval_dataset=grpo_eval_dataset,\n",
    ")\n",
    "\n",
    "print(f\"   Dataset size: {len(grpo_train_dataset)}\")\n",
    "print(f\"   Training for {GRPO_MAX_STEPS} steps\")\n",
    "print(f\"   Conservative settings to avoid tensor issues\")\n",
    "\n",
    "try:\n",
    "    # Run GRPO training\n",
    "    grpo_result = grpo_trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ GRPO TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Final training loss: {grpo_result.training_loss:.4f}\")\n",
    "    print(f\"‚úÖ Total steps completed: {grpo_result.global_step}\")\n",
    "    print(f\"‚úÖ Training time: {grpo_result.metrics.get('train_runtime', 'N/A')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå GRPO training failed: {e}\")\n",
    "    \n",
    "    # Automatic fallback with ultra-conservative settings\n",
    "    print(\"üîß Trying with ultra-conservative settings...\")\n",
    "    \n",
    "    # Update config\n",
    "    grpo_config.per_device_train_batch_size = 1\n",
    "    grpo_config.num_generations = 1\n",
    "    grpo_config.max_steps = 3\n",
    "    grpo_config.max_completion_length = 256\n",
    "    grpo_config.temperature = 0.7\n",
    "    \n",
    "    # Use smaller dataset\n",
    "    tiny_train_data = grpo_train_data[:min(10, len(grpo_train_data))]\n",
    "    tiny_dataset = Dataset.from_list(tiny_train_data)\n",
    "    \n",
    "    # Create new trainer\n",
    "    grpo_trainer = GRPOTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        reward_funcs=reward_functions,\n",
    "        args=grpo_config,\n",
    "        train_dataset=tiny_dataset,\n",
    "        eval_dataset=None,  # Skip eval for fallback\n",
    "    )\n",
    "    \n",
    "    grpo_result = grpo_trainer.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ GRPO TRAINING COMPLETED (Ultra-Conservative)!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"‚úÖ Used fallback settings with {len(tiny_dataset)} samples\")\n",
    "    print(f\"‚úÖ Training completed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing the trained model (standard generation)...\")\n",
    "\n",
    "# Test problem\n",
    "test_x0, test_v0 = 0.5, -0.3\n",
    "test_problem = f\"Control a double integrator system with initial state [position={test_x0:.2f}, velocity={test_v0:.2f}] to reach the origin (0,0) in {total_time:.2f} seconds using {STEPS} steps. Ensure all states remain within [-1,1] and controls within [-3,3].\"\n",
    "\n",
    "test_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": test_problem},\n",
    "]\n",
    "\n",
    "# Format for generation\n",
    "test_text = tokenizer.apply_chat_template(\n",
    "    test_messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "print(f\"   Prompt length: {len(test_text)} characters\")\n",
    "\n",
    "# Generate response using standard HuggingFace generation\n",
    "inputs = tokenizer(test_text, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LENGTH)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "print(\"   Generating response...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "# Decode response\n",
    "generated_tokens = outputs[0][len(inputs[\"input_ids\"][0]):]\n",
    "output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "print(f\"\\nüìù Model Response:\")\n",
    "print(\"=\"*60)\n",
    "print(output)\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check format\n",
    "has_reasoning = REASONING_START in output and REASONING_END in output\n",
    "has_controls = SOLUTION_START in output and SOLUTION_END in output\n",
    "\n",
    "print(f\"\\nüìä Response Analysis:\")\n",
    "print(f\"   Has reasoning tags: {has_reasoning}\")\n",
    "print(f\"   Has control tags: {has_controls}\")\n",
    "print(f\"   Response length: {len(output)} characters\")\n",
    "\n",
    "if has_controls:\n",
    "    # Extract controls\n",
    "    control_match = re.search(rf\"{SOLUTION_START}(.*?){SOLUTION_END}\", output, re.DOTALL)\n",
    "    if control_match:\n",
    "        try:\n",
    "            control_text = control_match.group(1).strip()\n",
    "            control_values = [float(x.strip()) for x in control_text.split(',')]\n",
    "            print(f\"   Extracted {len(control_values)} control values\")\n",
    "            print(f\"   Control range: [{min(control_values):.3f}, {max(control_values):.3f}]\")\n",
    "            \n",
    "            # Quick simulation\n",
    "            x, v = test_x0, test_v0\n",
    "            for u in control_values:\n",
    "                v = v + u * DT\n",
    "                x = x + v * DT\n",
    "            \n",
    "            final_error = np.sqrt(x**2 + v**2)\n",
    "            print(f\"   Final position: ({x:.4f}, {v:.4f})\")\n",
    "            print(f\"   Final error: {final_error:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Could not parse controls: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Testing completed (standard generation)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to save the model\n",
    "# print(\"üíæ Saving trained model...\")\n",
    "# model.save_lora(\"clean_grpo_model\")\n",
    "# print(\"‚úÖ Model saved as 'clean_grpo_model'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provides a clean, working GRPO implementation that:\n",
    "\n",
    "‚úÖ **Uses your proven working notebook approach** - exact same structure as `archive/Qwen3_(4B)-GRPO_control.ipynb`\n",
    "\n",
    "‚úÖ **Loads your existing dataset** - works with `datasets/di_train.pkl` and `datasets/di_eval.pkl`\n",
    "\n",
    "‚úÖ **Includes conservative tensor-safe settings** - avoids dimension mismatch issues\n",
    "\n",
    "‚úÖ **Has configurable parameters** - easy to switch between testing and production\n",
    "\n",
    "‚úÖ **Includes automatic fallback** - if training fails, automatically retries with ultra-conservative settings\n",
    "\n",
    "### To scale up for production:\n",
    "1. Set `USE_SMALL_DATASET = False`\n",
    "2. Increase `GRPO_MAX_STEPS` (e.g., to 50)\n",
    "3. Optionally increase `GRPO_NUM_GENERATIONS` (e.g., to 2 or 4)\n",
    "4. Enable wandb logging by changing `report_to=\"wandb\"`\n",
    "\n",
    "### The model should now generate proper control sequences with reasoning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
