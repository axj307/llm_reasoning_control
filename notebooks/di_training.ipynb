{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Integrator Training Notebook\n",
    "\n",
    "This notebook provides a clean interface for training and evaluating models on the Double Integrator system.\n",
    "\n",
    "**Sections:**\n",
    "1. Setup & Data Generation\n",
    "2. SFT Training (Optional)\n",
    "3. GRPO Training (Optional) \n",
    "4. SFT + GRPO Training (Combined)\n",
    "5. Model Evaluation\n",
    "6. Results Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration validation passed!\n",
      "Universal Control LLM configuration loaded from YAML files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 07-31 17:05:37 [importing.py:53] Triton module has been replaced with a placeholder.\n",
      "INFO 07-31 17:05:38 [__init__.py:239] Automatically detected platform cuda.\n",
      "‚úÖ All modules loaded successfully!\n",
      "Available systems: ['double_integrator', 'van_der_pol']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the parent directory of the notebooks folder\n",
    "notebook_dir = Path.cwd()\n",
    "if notebook_dir.name == 'notebooks':\n",
    "    parent_dir = notebook_dir.parent\n",
    "else:\n",
    "    # If we're already in the parent directory, use it\n",
    "    parent_dir = notebook_dir\n",
    "\n",
    "# Add parent directory to path and change to it\n",
    "sys.path.insert(0, str(parent_dir))\n",
    "os.chdir(parent_dir)\n",
    "\n",
    "from config import ALL_CONFIG, AVAILABLE_SYSTEMS\n",
    "from core.model_manager import UniversalModelManager\n",
    "from core.data_pipeline import UniversalDataGenerator\n",
    "from training.sft_training import train_sft_model, setup_universal_chat_template, save_sft_model\n",
    "from training.grpo_training import train_grpo_model, save_grpo_model\n",
    "from evaluation.inference import run_batch_inference\n",
    "from evaluation.metrics import compute_batch_metrics\n",
    "from evaluation.visualization import plot_comparison, plot_metrics_comparison\n",
    "from environments import get_system\n",
    "from data_utils import load_train_eval_datasets, list_available_datasets\n",
    "from gpu_utils import auto_gpu_config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(\"‚úÖ All modules loaded successfully!\")\n",
    "print(f\"Available systems: {AVAILABLE_SYSTEMS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Training system: double_integrator\n",
      "üìä Dataset: di\n",
      "üîß LoRA rank: 8\n",
      "üìè Max sequence length: 1024\n",
      "üî¢ Training mode: Reduced dataset\n",
      "   SFT samples: 50, max steps: 10\n",
      "   GRPO samples: 30, max steps: 5\n",
      "üí° Set USE_REDUCED_DATASET=False for full training\n",
      "‚ö†Ô∏è  Using conservative settings to avoid tensor dimension issues\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "SYSTEM_NAME = \"double_integrator\"\n",
    "DATASET_NAME = \"di\"  # Simple clean name\n",
    "LORA_RANK = 8\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "\n",
    "# Training Configuration (EDIT THESE PARAMETERS AS NEEDED)\n",
    "USE_REDUCED_DATASET = True  # Set to False for full dataset training\n",
    "SFT_SAMPLES = 50  # Number of samples for SFT training (reduced for stability)\n",
    "GRPO_SAMPLES = 30  # Number of samples for GRPO training (reduced for stability)\n",
    "SFT_MAX_STEPS = 10  # Max steps for SFT (reduced for stability)\n",
    "GRPO_MAX_STEPS = 5  # Max steps for GRPO (reduced for stability)\n",
    "\n",
    "print(f\"üéØ Training system: {SYSTEM_NAME}\")\n",
    "print(f\"üìä Dataset: {DATASET_NAME}\")\n",
    "print(f\"üîß LoRA rank: {LORA_RANK}\")\n",
    "print(f\"üìè Max sequence length: {MAX_SEQ_LENGTH}\")\n",
    "print(f\"üî¢ Training mode: {'Reduced dataset' if USE_REDUCED_DATASET else 'Full dataset'}\")\n",
    "if USE_REDUCED_DATASET:\n",
    "    print(f\"   SFT samples: {SFT_SAMPLES}, max steps: {SFT_MAX_STEPS}\")\n",
    "    print(f\"   GRPO samples: {GRPO_SAMPLES}, max steps: {GRPO_MAX_STEPS}\")\n",
    "    print(\"üí° Set USE_REDUCED_DATASET=False for full training\")\n",
    "    print(\"‚ö†Ô∏è  Using conservative settings to avoid tensor dimension issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation & Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Available datasets:\n",
      "   ‚Ä¢ di\n",
      "   ‚Ä¢ di_test\n"
     ]
    }
   ],
   "source": [
    "# Check available datasets\n",
    "print(\"üìÇ Available datasets:\")\n",
    "datasets = list_available_datasets(\"datasets\")  # Changed from \"../datasets\" to \"datasets\"\n",
    "if datasets:\n",
    "    for dataset in datasets:\n",
    "        print(f\"   ‚Ä¢ {dataset}\")\n",
    "else:\n",
    "    print(\"   No datasets found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è  Skipping data generation (set GENERATE_NEW_DATA=True to generate)\n"
     ]
    }
   ],
   "source": [
    "# Generate DI dataset (run this cell if you don't have the dataset)\n",
    "GENERATE_NEW_DATA = False  # Set to True to generate new data\n",
    "\n",
    "if GENERATE_NEW_DATA:\n",
    "    print(\"üîÑ Generating new Double Integrator dataset...\")\n",
    "    \n",
    "    generator = UniversalDataGenerator(\n",
    "        systems=[SYSTEM_NAME],\n",
    "        dt=ALL_CONFIG[\"system\"][\"dt\"],\n",
    "        steps=ALL_CONFIG[\"system\"][\"steps\"],\n",
    "        reasoning_start=ALL_CONFIG[\"system\"][\"reasoning_start\"],\n",
    "        reasoning_end=ALL_CONFIG[\"system\"][\"reasoning_end\"],\n",
    "        solution_start=ALL_CONFIG[\"system\"][\"solution_start\"],\n",
    "        solution_end=ALL_CONFIG[\"system\"][\"solution_end\"]\n",
    "    )\n",
    "    \n",
    "    # Generate 2000 samples (1800 train + 200 eval)\n",
    "    data = generator.generate_single_system_dataset(SYSTEM_NAME, 2000)\n",
    "    train_data, eval_data = generator.split_dataset(data, 0.9)\n",
    "    \n",
    "    # Save dataset\n",
    "    import pickle\n",
    "    os.makedirs(\"../datasets\", exist_ok=True)\n",
    "    \n",
    "    with open(f\"../datasets/{DATASET_NAME}_train.pkl\", 'wb') as f:\n",
    "        pickle.dump(train_data, f)\n",
    "    with open(f\"../datasets/{DATASET_NAME}_eval.pkl\", 'wb') as f:\n",
    "        pickle.dump(eval_data, f)\n",
    "    \n",
    "    print(f\"‚úÖ Generated and saved dataset: {DATASET_NAME}\")\n",
    "    print(f\"   üìà Train samples: {len(train_data)}\")\n",
    "    print(f\"   üìä Eval samples: {len(eval_data)}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping data generation (set GENERATE_NEW_DATA=True to generate)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from datasets/di_train.pkl (format: pickle)\n",
      "   ‚úÖ Loaded 1800 samples\n",
      "üìÇ Loading dataset from datasets/di_eval.pkl (format: pickle)\n",
      "   ‚úÖ Loaded 200 samples\n",
      "‚ö†Ô∏è  No info file found for dataset: datasets/di_train.pkl\n",
      "   üîç Filtered to 1800 samples for system 'double_integrator'\n",
      "   üîç Filtered to 200 samples for system 'double_integrator'\n",
      "üìä Dataset loaded: 1800 train, 200 eval samples\n",
      "‚úÖ Loaded dataset: di\n",
      "   üìà Train samples: 1800\n",
      "   üìä Eval samples: 200\n",
      "   ‚ÑπÔ∏è  Dataset info: {}\n"
     ]
    }
   ],
   "source": [
    "# Load existing dataset\n",
    "try:\n",
    "    train_data, eval_data, dataset_info = load_train_eval_datasets(\n",
    "        DATASET_NAME, \"datasets\", SYSTEM_NAME  # Changed from \"../datasets\" to \"datasets\"\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded dataset: {DATASET_NAME}\")\n",
    "    print(f\"   üìà Train samples: {len(train_data)}\")\n",
    "    print(f\"   üìä Eval samples: {len(eval_data)}\")\n",
    "    print(f\"   ‚ÑπÔ∏è  Dataset info: {dataset_info.get('config', {})}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load dataset: {e}\")\n",
    "    print(\"üí° Set GENERATE_NEW_DATA=True in the cell above to generate the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Setting up GPU and model...\n",
      "üñ•Ô∏è  GPU Status:\n",
      "   GPU 0:  22491MB free /  81559MB total üî¥ BUSY\n",
      "üßπ Cleared GPU memory cache\n",
      "üéØ Selected GPU 0: 22491MB free (72.4% used)\n",
      "üöÄ Using GPU 0: NVIDIA H100 80GB HBM3\n",
      "üñ•Ô∏è  Selected GPU: 0\n",
      "üìå Using specified GPU 0\n",
      "üöÄ Using GPU 0: NVIDIA H100 80GB HBM3\n",
      "üöÄ Loading model: unsloth/Qwen3-4B-Base\n",
      "   Max sequence length: 1024\n",
      "   LoRA rank: 8\n",
      "   GPU memory utilization: 0.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.6.1: Fast Qwen3 patching. Transformers: 4.51.3. vLLM: 0.8.5.post1.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "üîß Applying LoRA configuration...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.6.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model setup completed successfully!\n",
      "‚úÖ Model setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup GPU and model manager\n",
    "print(\"üéØ Setting up GPU and model...\")\n",
    "\n",
    "# Auto-select best GPU\n",
    "gpu_config = auto_gpu_config()\n",
    "print(f\"üñ•Ô∏è  Selected GPU: {gpu_config['gpu_id']}\")\n",
    "\n",
    "# Create model manager\n",
    "manager = UniversalModelManager(ALL_CONFIG[\"model\"][\"base_model_name\"])\n",
    "\n",
    "# Setup model\n",
    "model, tokenizer = manager.setup_model(\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    lora_rank=LORA_RANK,\n",
    "    gpu_id=gpu_config['gpu_id'],\n",
    "    auto_select_gpu=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  Chat template setup is now handled automatically by training functions\n",
      "‚úÖ No manual chat template setup needed\n"
     ]
    }
   ],
   "source": [
    "# Chat Template Setup (Optional - automatically handled by training functions)\n",
    "# Note: The training functions will automatically set up the chat template if needed\n",
    "# This cell is kept for reference but is no longer required\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Chat template setup is now handled automatically by training functions\")\n",
    "print(\"‚úÖ No manual chat template setup needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SFT Training (Optional)\n",
    "\n",
    "Run this section to train only the SFT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è  Skipping SFT-only training (set RUN_SFT_ONLY=True to run)\n"
     ]
    }
   ],
   "source": [
    "# SFT Training\n",
    "RUN_SFT_ONLY = False  # Set to True to run SFT training only\n",
    "\n",
    "if RUN_SFT_ONLY:\n",
    "    print(\"üöÄ Starting SFT Training...\")\n",
    "    \n",
    "    # Update SFT config\n",
    "    sft_config = ALL_CONFIG[\"sft\"].copy()\n",
    "    sft_config[\"output_dir\"] = f\"../temp_training/{SYSTEM_NAME}/sft\"\n",
    "    \n",
    "    # Train SFT\n",
    "    sft_result = train_sft_model(\n",
    "        manager, train_data, eval_data, sft_config\n",
    "    )\n",
    "    \n",
    "    # Save SFT model\n",
    "    sft_save_path = save_sft_model(\n",
    "        manager, [SYSTEM_NAME], sft_result[\"metrics\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ SFT model saved to: {sft_save_path}\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping SFT-only training (set RUN_SFT_ONLY=True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. SFT + GRPO Training (Combined)\n",
    "\n",
    "Run this section to train both SFT and GRPO models in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Combined SFT + GRPO Training...\n",
      "üîß Using reduced dataset: SFT=50, GRPO=30 samples\n",
      "\n",
      "============================================================\n",
      "üìö SFT TRAINING PHASE\n",
      "============================================================\n",
      "   Training samples: 50\n",
      "   Evaluation samples: 10\n",
      "   Max steps: 10 (reduced for testing)\n",
      "‚ö†Ô∏è  Chat template not set, setting up default template...\n",
      "‚úÖ Chat template set up successfully\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0653a6e332b0482a9b233ad0a8d714b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7716e8fa594042efb1f926a46c957486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 50\n",
      "Evaluation dataset size: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97a897285b1849e383121cd0ae6b1e0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367dc8fda81e4600bc8ecf22cbc4bf73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting SFT training...\n",
      "   Dataset size: 50\n",
      "   Batch size: 2\n",
      "   Max sequence length: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 50 | Num Epochs = 1 | Total steps = 10\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 1 x 1) = 2\n",
      " \"-____-\"     Trainable parameters = 16,515,072/4,000,000,000 (0.41% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "SFT training completed!\n",
      "Model saved to models/single_system/double_integrator/sft/run_20250731_170616\n",
      "‚úÖ SFT model saved to: models/single_system/double_integrator/sft/run_20250731_170616\n",
      "\n",
      "============================================================\n",
      "üéÆ GRPO TRAINING PHASE (TENSOR-SAFE)\n",
      "============================================================\n",
      "   Training samples: 30\n",
      "   Evaluation samples: 5\n",
      "   Max steps: 5 (reduced for testing)\n",
      "   ‚ö†Ô∏è  Using tensor-safe settings: batch_size=1, num_generations=1\n",
      "üîß GRPO sequence length configuration:\n",
      "   Model max length: 1024\n",
      "   Max completion length: 512\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca89d42f852438c82b84c0b5adc7cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba01fb8e1ab4398bcd028042d93817a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 30\n",
      "Evaluation dataset size: 5\n",
      "Using 3 reward functions\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The global train batch size (1 x 1) must be evenly divisible by the number of generations per prompt (1). Given the current train batch size, the valid values for the number of generations are: [].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   ‚ö†Ô∏è  Using tensor-safe settings: batch_size=1, num_generations=1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# FIXED: Use the improved GRPO training function with tensor fixes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m grpo_result = \u001b[43mtrain_grpo_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrpo_train_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrpo_eval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrpo_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m    \u001b[49m\u001b[43mALL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_start\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43mALL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43mALL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msolution_start\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43mALL_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msolution_end\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m grpo_save_path = save_grpo_model(\n\u001b[32m     87\u001b[39m     manager, [SYSTEM_NAME], grpo_result[\u001b[33m\"\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     88\u001b[39m )\n\u001b[32m     90\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ GRPO model saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrpo_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/amitjain/project/Unsloth/llm_reasoning_control/training/grpo_training.py:194\u001b[39m, in \u001b[36mtrain_grpo_model\u001b[39m\u001b[34m(model_manager, train_data, eval_data, training_config, reasoning_start, reasoning_end, solution_start, solution_end)\u001b[39m\n\u001b[32m    191\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(reward_functions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m reward functions\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Create trainer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m trainer = \u001b[43mGRPOTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrpo_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting GRPO training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    204\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/unsloth_env/lib/python3.11/site-packages/unsloth/trainer.py:210\u001b[39m, in \u001b[36m_backwards_compatible_trainer.<locals>.new_init\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m] = config\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[43moriginal_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/amitjain/project/Unsloth/llm_reasoning_control/unsloth_compiled_cache/UnslothGRPOTrainer.py:1591\u001b[39m, in \u001b[36mUnslothGRPOTrainer.__init__\u001b[39m\u001b[34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, peft_config, **kwargs)\u001b[39m\n\u001b[32m   1588\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlogging_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PatchRLStatistics\n\u001b[32m   1589\u001b[39m PatchRLStatistics(\u001b[33m'\u001b[39m\u001b[33mgrpo_trainer\u001b[39m\u001b[33m'\u001b[39m, other_metrics)\n\u001b[32m-> \u001b[39m\u001b[32m1591\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m   1592\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1593\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_funcs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1594\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1596\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1597\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1598\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_processing_classes\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward_processing_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1599\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1600\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpeft_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1601\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mneftune_hook_handle\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1602\u001b[39m     \u001b[38;5;28mself\u001b[39m.neftune_hook_handle.remove()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/orcd/home/002/amitjain/project/Unsloth/llm_reasoning_control/unsloth_compiled_cache/UnslothGRPOTrainer.py:964\u001b[39m, in \u001b[36m_UnslothGRPOTrainer.__init__\u001b[39m\u001b[34m(self, model, reward_funcs, args, train_dataset, eval_dataset, processing_class, reward_processing_classes, callbacks, optimizers, peft_config)\u001b[39m\n\u001b[32m    962\u001b[39m possible_values = [n_gen \u001b[38;5;28;01mfor\u001b[39;00m n_gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m2\u001b[39m, global_batch_size + \u001b[32m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (global_batch_size) % n_gen == \u001b[32m0\u001b[39m]\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_generations \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m possible_values:\n\u001b[32m--> \u001b[39m\u001b[32m964\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    965\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe global train batch size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_processes\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m x \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.per_device_train_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) must be evenly \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    966\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdivisible by the number of generations per prompt (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_generations\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m). Given the current train \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    967\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbatch size, the valid values for the number of generations are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpossible_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    968\u001b[39m     )\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.eval_strategy != \u001b[33m\"\u001b[39m\u001b[33mno\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    970\u001b[39m     global_batch_size = args.per_device_eval_batch_size * num_processes\n",
      "\u001b[31mValueError\u001b[39m: The global train batch size (1 x 1) must be evenly divisible by the number of generations per prompt (1). Given the current train batch size, the valid values for the number of generations are: []."
     ]
    }
   ],
   "source": [
    "# Combined SFT + GRPO Training - FIXED VERSION WITH TENSOR DIMENSION FIXES\n",
    "RUN_COMBINED_TRAINING = True  # Set to True to run full training pipeline\n",
    "\n",
    "if RUN_COMBINED_TRAINING:\n",
    "    print(\"üöÄ Starting Combined SFT + GRPO Training...\")\n",
    "    \n",
    "    # Prepare datasets based on configuration\n",
    "    if USE_REDUCED_DATASET:\n",
    "        print(f\"üîß Using reduced dataset: SFT={SFT_SAMPLES}, GRPO={GRPO_SAMPLES} samples\")\n",
    "        sft_train_data = train_data[:SFT_SAMPLES]\n",
    "        sft_eval_data = eval_data[:min(10, len(eval_data))]  # Small eval set\n",
    "        grpo_train_data = train_data[:GRPO_SAMPLES] \n",
    "        grpo_eval_data = eval_data[:min(5, len(eval_data))]  # Small eval set\n",
    "    else:\n",
    "        print(\"üìä Using full dataset\")\n",
    "        sft_train_data = train_data\n",
    "        sft_eval_data = eval_data\n",
    "        grpo_train_data = train_data\n",
    "        grpo_eval_data = eval_data\n",
    "    \n",
    "    # === SFT Phase ===\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìö SFT TRAINING PHASE\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"   Training samples: {len(sft_train_data)}\")\n",
    "    print(f\"   Evaluation samples: {len(sft_eval_data)}\")\n",
    "    \n",
    "    sft_config = ALL_CONFIG[\"sft\"].copy()\n",
    "    sft_config[\"output_dir\"] = f\"temp_training/{SYSTEM_NAME}/sft\"\n",
    "    \n",
    "    if USE_REDUCED_DATASET:\n",
    "        # Reduced training for debugging/testing\n",
    "        sft_config.update({\n",
    "            \"num_train_epochs\": 1,\n",
    "            \"max_steps\": SFT_MAX_STEPS,  # Use configured max steps\n",
    "            \"logging_steps\": max(1, SFT_MAX_STEPS // 5),  # Log every 20%\n",
    "            \"save_steps\": SFT_MAX_STEPS + 10,  # Save at end\n",
    "            \"per_device_train_batch_size\": 2,  # Conservative batch size\n",
    "        })\n",
    "        print(f\"   Max steps: {SFT_MAX_STEPS} (reduced for testing)\")\n",
    "    \n",
    "    sft_result = train_sft_model(\n",
    "        manager, sft_train_data, sft_eval_data, sft_config\n",
    "    )\n",
    "    \n",
    "    sft_save_path = save_sft_model(\n",
    "        manager, [SYSTEM_NAME], sft_result[\"metrics\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ SFT model saved to: {sft_save_path}\")\n",
    "    \n",
    "    # === GRPO Phase ===\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéÆ GRPO TRAINING PHASE (TENSOR-SAFE)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"   Training samples: {len(grpo_train_data)}\")\n",
    "    print(f\"   Evaluation samples: {len(grpo_eval_data)}\")\n",
    "    \n",
    "    grpo_config = ALL_CONFIG[\"grpo\"].copy()\n",
    "    grpo_config[\"output_dir\"] = f\"temp_training/{SYSTEM_NAME}/grpo\"\n",
    "    \n",
    "    if USE_REDUCED_DATASET:\n",
    "        # Conservative settings to avoid tensor dimension issues\n",
    "        grpo_config.update({\n",
    "            \"max_steps\": GRPO_MAX_STEPS,  # Use configured max steps\n",
    "            \"num_generations\": 1,  # Single generation to avoid tensor issues\n",
    "            \"per_device_train_batch_size\": 1,  # Single batch to avoid issues\n",
    "            \"gradient_accumulation_steps\": 2,  # Compensate with accumulation\n",
    "            \"max_completion_length\": 256,  # Smaller completion length\n",
    "            \"temperature\": 0.8,  # Lower temperature for stability\n",
    "            \"logging_steps\": 1,\n",
    "            \"save_steps\": GRPO_MAX_STEPS + 10,  # Save at end\n",
    "        })\n",
    "        print(f\"   Max steps: {GRPO_MAX_STEPS} (reduced for testing)\")\n",
    "        print(f\"   ‚ö†Ô∏è  Using tensor-safe settings: batch_size=1, num_generations=1\")\n",
    "    \n",
    "    # FIXED: Use the improved GRPO training function with tensor fixes\n",
    "    grpo_result = train_grpo_model(\n",
    "        manager, grpo_train_data, grpo_eval_data, grpo_config,\n",
    "        ALL_CONFIG[\"system\"][\"reasoning_start\"],\n",
    "        ALL_CONFIG[\"system\"][\"reasoning_end\"],\n",
    "        ALL_CONFIG[\"system\"][\"solution_start\"],\n",
    "        ALL_CONFIG[\"system\"][\"solution_end\"]\n",
    "    )\n",
    "    \n",
    "    grpo_save_path = save_grpo_model(\n",
    "        manager, [SYSTEM_NAME], grpo_result[\"metrics\"]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ GRPO model saved to: {grpo_save_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"üìç SFT model: {sft_save_path}\")\n",
    "    print(f\"üìç GRPO model: {grpo_save_path}\")\n",
    "    \n",
    "    if USE_REDUCED_DATASET:\n",
    "        print(\"\\nüí° Training completed with conservative tensor-safe settings\")\n",
    "        print(\"üîß To scale up for production:\")\n",
    "        print(\"   1. In cell 3, set USE_REDUCED_DATASET = False\")\n",
    "        print(\"   2. Increase SFT_SAMPLES, GRPO_SAMPLES as needed\")\n",
    "        print(\"   3. The training will auto-adjust if tensor issues occur\")\n",
    "        print(\"   4. Or use SLURM scripts for production training\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping combined training (set RUN_COMBINED_TRAINING=True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate your trained models on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "RUN_EVALUATION = True  # Set to True to run evaluation\n",
    "EVALUATE_SFT = True   # Set to True to evaluate SFT model\n",
    "EVALUATE_GRPO = True  # Set to True to evaluate GRPO model\n",
    "\n",
    "if RUN_EVALUATION:\n",
    "    print(\"üìä Starting Model Evaluation...\")\n",
    "    \n",
    "    # Load models for evaluation\n",
    "    eval_manager = UniversalModelManager()\n",
    "    \n",
    "    if EVALUATE_SFT:\n",
    "        print(\"\\nüîç Evaluating SFT Model...\")\n",
    "        try:\n",
    "            sft_model, sft_tokenizer, sft_lora, sft_metadata = eval_manager.load_single_system_model(\n",
    "                SYSTEM_NAME, training_type=\"sft\"  # Changed from model_type to training_type\n",
    "            )\n",
    "            \n",
    "            # Generate test cases\n",
    "            system = get_system(SYSTEM_NAME)()\n",
    "            test_cases = []\n",
    "            for _ in range(10):  # 10 test cases\n",
    "                initial_state = system.generate_random_initial_state()\n",
    "                test_cases.append(tuple(initial_state))\n",
    "            \n",
    "            # Run inference\n",
    "            from vllm import SamplingParams\n",
    "            sampling_params = SamplingParams(\n",
    "                temperature=0.7,\n",
    "                top_k=50,\n",
    "                max_tokens=1024\n",
    "            )\n",
    "            \n",
    "            sft_results = run_batch_inference(\n",
    "                sft_model, sft_tokenizer, SYSTEM_NAME, test_cases,\n",
    "                lora_request=sft_lora,\n",
    "                sampling_params=sampling_params\n",
    "            )\n",
    "            \n",
    "            # Compute metrics\n",
    "            sft_metrics = compute_batch_metrics(sft_results)\n",
    "            \n",
    "            print(f\"‚úÖ SFT Evaluation Results:\")\n",
    "            print(f\"   Success rate: {sft_metrics['success_rate']:.2%}\")\n",
    "            print(f\"   Mean performance: {sft_metrics['mean_performance_score']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå SFT evaluation failed: {e}\")\n",
    "    \n",
    "    if EVALUATE_GRPO:\n",
    "        print(\"\\nüîç Evaluating GRPO Model...\")\n",
    "        try:\n",
    "            grpo_model, grpo_tokenizer, grpo_lora, grpo_metadata = eval_manager.load_single_system_model(\n",
    "                SYSTEM_NAME, training_type=\"grpo\"  # Changed from model_type to training_type\n",
    "            )\n",
    "            \n",
    "            # Run inference\n",
    "            grpo_results = run_batch_inference(\n",
    "                grpo_model, grpo_tokenizer, SYSTEM_NAME, test_cases,\n",
    "                lora_request=grpo_lora,\n",
    "                sampling_params=sampling_params\n",
    "            )\n",
    "            \n",
    "            # Compute metrics\n",
    "            grpo_metrics = compute_batch_metrics(grpo_results)\n",
    "            \n",
    "            print(f\"‚úÖ GRPO Evaluation Results:\")\n",
    "            print(f\"   Success rate: {grpo_metrics['success_rate']:.2%}\")\n",
    "            print(f\"   Mean performance: {grpo_metrics['mean_performance_score']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå GRPO evaluation failed: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  Skipping evaluation (set RUN_EVALUATION=True to run)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≠Ô∏è  No results to visualize (run evaluation first)\n"
     ]
    }
   ],
   "source": [
    "# Plot results if evaluation was run\n",
    "if RUN_EVALUATION and EVALUATE_GRPO and 'grpo_results' in locals():\n",
    "    print(\"üìà Generating visualizations...\")\n",
    "    \n",
    "    # Plot trajectory comparison\n",
    "    fig1 = plot_comparison(grpo_results)\n",
    "    if fig1:\n",
    "        plt.figure(fig1.number)\n",
    "        plt.suptitle(f\"Double Integrator GRPO Model Results\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    fig2 = plot_metrics_comparison(grpo_results)\n",
    "    if fig2:\n",
    "        plt.figure(fig2.number)\n",
    "        plt.suptitle(f\"Double Integrator Performance Metrics\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    print(\"‚úÖ Visualizations complete!\")\n",
    "else:\n",
    "    print(\"‚è≠Ô∏è  No results to visualize (run evaluation first)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "This notebook provides a complete workflow for training and evaluating Double Integrator control models:\n",
    "\n",
    "- **Data Generation**: Create clean DI dataset (1800 train + 200 eval)\n",
    "- **SFT Training**: Supervised fine-tuning for basic control knowledge\n",
    "- **GRPO Training**: Reinforcement learning for optimal control\n",
    "- **Evaluation**: Test model performance on unseen data\n",
    "- **Visualization**: Plot trajectories and performance metrics\n",
    "\n",
    "**Model Outputs:**\n",
    "- SFT model: `models/single_system/double_integrator/sft/latest/`\n",
    "- GRPO model: `models/single_system/double_integrator/grpo/latest/`\n",
    "\n",
    "**Next Steps:**\n",
    "- Use the VDP training notebook for Van der Pol oscillator\n",
    "- Use the universal training notebook for multi-system models\n",
    "- Load trained models in other notebooks for further analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
