# GRPO Convergence Fix - January 22, 2025

## Problem Analysis

**Core Issue**: GRPO was failing to improve beyond the SFT baseline despite good initial control policies from supervised fine-tuning. Training rewards remained consistently negative (-17 to -42), indicating the reward function was punishment-focused rather than progress-focused.

### Key Observations:
- **SFT Working Perfectly**: Model learned good control policies from optimal LQR demonstrations
- **GRPO Failing to Improve**: Rewards stayed negative, no learning progression observed
- **Negative Reward Dominance**: LQR cost (-190 to -440 before scaling) overwhelmed all positive signals
- **Missing Progress Signals**: No reward for incremental improvements from the good SFT baseline

## Root Cause Analysis

### Previous Reward Function Issues:
1. **Punishment-Focused Design**: Started from negative baseline, penalized imperfection
2. **LQR Cost Domination**: -10 to -50 reward overwhelmed terminal bonuses
3. **All-or-Nothing Terminal Rewards**: Only rewarded near-perfect final states (< 0.05 error)
4. **No Progress Recognition**: Failed to reward improvements from initial state
5. **Discrete Reward Steps**: Sharp thresholds instead of smooth reward landscapes

## Comprehensive Solution Implementation

### 1. **Progress-Focused Reward System** 

**NEW APPROACH**: Start positive, reward incremental improvements

```python
# === PROGRESS-FOCUSED REWARD SYSTEM ===
baseline_reward = 50.0                    # Positive foundation (+50)

# 1. DISTANCE PROGRESS REWARD (0 to +30)
progress_ratio = (initial_distance - final_error) / initial_distance
distance_progress_reward = 30.0 * progress_ratio

# 2. PROXIMITY REWARD (0 to +15) - staying close to origin
avg_distance = np.mean([np.sqrt(pos**2 + vel**2) for pos, vel in states])
proximity_reward = 15.0 * np.exp(-2.0 * avg_distance)

# 3. TERMINAL EXCELLENCE BONUS (0 to +25) - smooth continuous
terminal_bonus = 25.0 * np.exp(-10.0 * final_error)

# 4. CONTROL EFFICIENCY (0 to +10)
efficiency_reward = 10.0 * np.exp(-control_effort)

# 5. TRAJECTORY SMOOTHNESS (0 to +8)
smoothness_reward = 8.0 * np.exp(-2.0 * avg_change)

# 6. NORMALIZED LQR COST (-15 to 0) - bounded, not dominating
normalized_lqr = -np.tanh(total_cost / 200.0) * 15.0
```

**Expected Range**: 30-140 (mostly positive with clear improvement signals)

### 2. **GRPO Hyperparameter Optimization**

**Optimized for SFTâ†’GRPO Pipeline**:

```python
{
    "temperature": 1.5,        # Increased from 1.0 for better exploration
    "learning_rate": 1e-5,     # Increased from 5e-6 for faster improvement
    "lr_scheduler_type": "cosine",  # Changed from linear for smoother convergence
    "top_p": 0.9,             # Reduced from 1.0 for more focused sampling
    "beta": 0.01,             # Added KL penalty to prevent policy collapse
}
```

**Rationale**: 
- Higher temperature and learning rate since starting from good SFT baseline
- More focused sampling and KL regularization for stable improvement

### 3. **Enhanced MPC Skip Functionality**

**Fixed Issues**:
- Added debug output to clearly show MPC status
- Improved argument parsing for `--skip-mcp` flag
- Fixed statistics calculation for skipped MPC evaluations

```bash
# SLURM script usage:
python scripts/evaluate_model.py \
    --model-path "$GRPO_MODEL" \
    --num-cases 10 \
    --save-dir "$OUTPUT_DIR/grpo_evaluation" \
    --skip-mcp  # 50x faster evaluation
```

## Expected Results

### **Training Behavior**:
- **Rewards**: Should move from 30-50 range toward 100-140 range
- **Clear Learning Signal**: Progress rewards should show steady improvement
- **Convergence**: Should converge within 100-200 steps (was not converging)

### **Control Performance**:
- **Build on SFT Foundation**: Improve upon already-decent control policies
- **Trajectory Convergence**: Generate trajectories that reach origin consistently
- **Policy Refinement**: Smoother, more efficient control sequences

### **Reward Component Breakdown** (Debug Output):
```
ðŸŽ¯ REWARD BREAKDOWN: baseline=50.0, progress=18.5, proximity=8.2, 
terminal=15.3, efficiency=6.8, smoothness=4.2, lqr=-8.5, 
constraints=0.0, validity=5.0 â†’ TOTAL=99.5
```

## Key Insights

### **Why Previous Approach Failed**:
1. **Model felt "unsuccessful"** with consistently negative rewards
2. **No guidance for improvement** - only punishment for imperfection  
3. **SFT baseline couldn't be built upon** - GRPO didn't know what "better" meant

### **Why New Approach Should Work**:
1. **Positive reinforcement foundation** - model feels successful
2. **Clear progress signals** - rewards improvements from initial state
3. **Smooth reward landscapes** - guides learning instead of punishing
4. **Builds on SFT strength** - refines already-good policies

## Files Modified

### Core Implementation:
- `training/grpo_training.py`: Complete reward function overhaul
- `scripts/evaluate_model.py`: Enhanced MCP skip functionality 

### Documentation:
- `docs/ai/grpo_convergence_fix_2025_01_22.md`: This comprehensive guide
- `docs/ai/changelog.md`: Updated with convergence fix details

## Testing Plan

### **Validation Metrics**:
1. **Training Rewards**: Should become positive and increase over time
2. **Trajectory Quality**: Improved convergence to origin vs SFT baseline
3. **Learning Convergence**: Clear improvement within 100-200 steps
4. **Control Refinement**: Smoother, more efficient control policies

### **Success Criteria**:
- **Rewards > 80**: Good performance range
- **Final Error < 0.1**: Consistent origin convergence
- **Clear Learning Curve**: Visible reward improvement during training

## Future Improvements

### **Advanced Reward Shaping**:
- Curriculum learning: Start with easier initial conditions
- Adaptive reward scaling based on performance
- Multi-objective optimization (accuracy + efficiency)

### **Training Optimization**:
- Dynamic temperature scheduling
- Adaptive learning rates based on reward progress
- Early stopping when convergence achieved

This fix transforms GRPO from a punishment-based system to a progress-focused learning algorithm that can successfully build upon the strong SFT foundation for control tasks.