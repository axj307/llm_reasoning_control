#!/bin/bash
#SBATCH --job-name=debug_llm_outputs
#SBATCH --output=slurm_logs/debug_llm_outputs_%j.out
#SBATCH --error=slurm_logs/debug_llm_outputs_%j.err
#SBATCH --time=00:30:00
#SBATCH --partition=pi_linaresr
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=24G
#SBATCH --gres=gpu:1

echo "ğŸ” LLM INPUT/OUTPUT DEBUG TOOL"
echo "============================================================"
echo "Inspecting actual question-answer pairs from GRPO training"
echo "============================================================"

# Environment setup
source ~/.bashrc
conda activate unsloth_env
cd /orcd/home/002/amitjain/project/Unsloth/llm_reasoning_control_refactored

# Set up timestamp for results
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
OUTPUT_DIR="results/debug_outputs_${TIMESTAMP}"
mkdir -p "$OUTPUT_DIR"

echo "ğŸ“ Debug output directory: $OUTPUT_DIR"

echo ""
echo "ğŸ“Š PHASE 1: DATASET FORMAT DEBUG"
echo "============================================================"
echo "ğŸ” Examining actual dataset format..."

python -c "
import sys
sys.path.append('.')
import pickle
from pathlib import Path
import json

print('ğŸ” Dataset Format Analysis:')
print('=' * 40)

# Check available datasets
dataset_files = [
    'datasets/di_train.pkl',
    'datasets/di_quick_train.pkl', 
    'datasets/universal_train.pkl'
]

for dataset_file in dataset_files:
    if Path(dataset_file).exists():
        print(f'\\nğŸ“‚ {dataset_file}:')
        with open(dataset_file, 'rb') as f:
            data = pickle.load(f)
        
        print(f'   ğŸ“Š Total samples: {len(data)}')
        if len(data) > 0:
            sample = data[0]
            print(f'   ğŸ”‘ Keys: {list(sample.keys())}')
            
            if 'Messages' in sample:
                messages = sample['Messages']
                print(f'   ğŸ’¬ Messages: {len(messages)} parts')
                
                for j, msg in enumerate(messages):
                    role = msg.get('role', 'unknown')
                    content = msg.get('content', '')
                    print(f'     {j+1}. {role}: {len(content)} chars')
                
                # Show full example
                print(f'\\nğŸ“ FULL SAMPLE EXAMPLE:')
                print('   System prompt:', messages[0]['content'][:200] + '...')
                print('   User question:', messages[1]['content'])
                print('   Assistant answer:', messages[2]['content'][:500] + '...')
    else:
        print(f'âŒ {dataset_file} not found')
"

echo ""
echo "ğŸ“Š PHASE 2: LIVE MODEL TESTING"
echo "============================================================"
echo "ğŸ¤– Testing current model with sample questions..."

python debug_grpo_outputs.py > "$OUTPUT_DIR/live_model_test.txt" 2>&1

echo "âœ… Live model test completed!"
echo "ğŸ“„ Results saved to: $OUTPUT_DIR/live_model_test.txt"

echo ""
echo "ğŸ“Š PHASE 3: EVALUATION PIPELINE DEBUG"
echo "============================================================"
echo "ğŸ§ª Testing evaluation pipeline with debug output..."

python -c "
import sys
sys.path.append('.')
from evaluation.inference import run_inference
from core.model_manager import ModelManager
import torch

print('ğŸ§ª Evaluation Pipeline Debug:')
print('=' * 40)

try:
    # Try to load any available model
    manager = ModelManager()
    
    # Check for models
    model_paths = [
        'models/single_system/double_integrator/sft/latest',
        'models/universal/sft/latest'
    ]
    
    model_found = False
    for model_path in model_paths:
        from pathlib import Path
        if Path(model_path).exists():
            print(f'âœ… Found model: {model_path}')
            try:
                model, tokenizer, lora_request, metadata = manager.load_checkpoint(model_path)
                print(f'âœ… Model loaded successfully')
                
                # Test inference with debug output
                print('\\nğŸ” Testing inference with debug output:')
                result = run_inference(
                    model, tokenizer, 'double_integrator',
                    initial_state=(1.5, 0.8),
                    dt=0.1,
                    steps=5,
                    lora_request=lora_request
                )
                
                print('ğŸ“ INFERENCE INPUT:')
                print('   System: double_integrator')
                print('   Initial state: (1.5, 0.8)')
                print('   Steps: 5')
                
                print('\\nğŸ¤– INFERENCE OUTPUT:')
                print(f'   Reasoning: {result.get(\"reasoning\", \"None\")[:200]}...')
                print(f'   Controls: {result.get(\"controls\", \"None\")}')
                print(f'   Success: {result.get(\"success\", False)}')
                
                model_found = True
                break
                
            except Exception as e:
                print(f'âŒ Error loading {model_path}: {e}')
    
    if not model_found:
        print('âŒ No trained models found. Train a model first.')
        
except Exception as e:
    print(f'âŒ Error in evaluation debug: {e}')
" > "$OUTPUT_DIR/evaluation_debug.txt" 2>&1

echo "âœ… Evaluation debug completed!"
echo "ğŸ“„ Results saved to: $OUTPUT_DIR/evaluation_debug.txt"

echo ""
echo "ğŸ“Š PHASE 4: GENERATE SAMPLE OUTPUTS REPORT"
echo "============================================================"
echo "ğŸ“Š Creating comprehensive sample outputs report..."

python -c "
print('ğŸ“Š COMPREHENSIVE LLM OUTPUT ANALYSIS')
print('=' * 50)
print()

print('ğŸ¯ This debug run checked:')
print('1. âœ… Dataset format and structure')
print('2. âœ… Live model question-answer generation')  
print('3. âœ… Evaluation pipeline inference')
print('4. âœ… Format consistency verification')
print()

print('ğŸ“ All detailed results saved to: $OUTPUT_DIR')
print()

print('ğŸ” TO VERIFY YOUR TRAINING:')
print('1. Check dataset format matches expectations')
print('2. Verify model generates proper <REASONING> and <CONTROLS> tags')
print('3. Ensure control values are parseable as numbers')
print('4. Confirm evaluation pipeline works with same format')
print()

print('ğŸ’¡ NEXT STEPS:')
print('1. Review the output files for any format mismatches')
print('2. If formats look good, your training should work correctly')
print('3. If issues found, adjust dataset generation or model prompts')
print()
" > "$OUTPUT_DIR/summary_report.txt"

echo "âœ… Sample outputs report generated!"

echo ""
echo "ğŸ“‹ DEBUG SUMMARY"
echo "============================================================"
echo "ğŸ‰ LLM OUTPUT DEBUG COMPLETED!"
echo ""
echo "ğŸ“ All results saved to: $OUTPUT_DIR"
echo ""
echo "ğŸ“„ Key files to review:"
echo "   ğŸ“Š live_model_test.txt - Live model question-answer pairs"
echo "   ğŸ§ª evaluation_debug.txt - Evaluation pipeline testing"
echo "   ğŸ“‹ summary_report.txt - Overall analysis summary"
echo ""
echo "ğŸ” Review these files to verify:"
echo "   âœ… Question format matches your expectations"
echo "   âœ… Model responses have proper <REASONING> and <CONTROLS> tags"
echo "   âœ… Control values are parseable and reasonable"
echo "   âœ… Evaluation pipeline processes outputs correctly"
echo ""
echo "ğŸš€ USE THIS INFO TO VALIDATE YOUR TRAINING PIPELINE!"
echo "============================================================"